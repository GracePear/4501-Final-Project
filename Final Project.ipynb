{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project\n",
    "\n",
    "import os\n",
    "import re\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import pyarrow.parquet as pq\n",
    "import geopandas as gpd\n",
    "import warnings\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TLC_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "### Load Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58708809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html() -> str:\n",
    "    response = requests.get(TLC_URL)\n",
    "    html = response.content\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d035e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_HVFHV_parquet_links() -> List[str]:\n",
    "    \"\"\"\n",
    "    Finds and returns a list of URLs of High Volume FHV parquet files.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: a list of strings representing URLs of High Volume FHV parquet files.\n",
    "    \"\"\"\n",
    "    parquet_links = list()\n",
    "    \n",
    "    html = get_html()\n",
    "    soup = bs4.BeautifulSoup(html, \"html.parser\")\n",
    "    links = soup.find_all(\"a\")\n",
    "    pattern = re.compile(r\"High Volume For-Hire Vehicle Trip Records\")\n",
    "    for link in links:\n",
    "        title = link.get('title')\n",
    "        if title != None:\n",
    "            match = pattern.search(title)\n",
    "            if match:\n",
    "                parquet_links.append(link.get('href'))\n",
    "    return parquet_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31899585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_HVFHV_parquet_files() -> None:\n",
    "    \"\"\"\n",
    "    Downloads High Volume FHV parquet files from URLs found by 'find_HVFHV_parquet_links'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    taxi_files = find_HVFHV_parquet_links()\n",
    "    for file_url in taxi_files:\n",
    "        file_url = file_url.replace(' ', '')\n",
    "        name = file_url.split('trip-data/')[1]\n",
    "        # Check if the file already exists\n",
    "        if os.path.exists(name):\n",
    "            pass\n",
    "        else:\n",
    "            response = requests.get(file_url, stream=True)\n",
    "            with open(name, \"wb\") as f:\n",
    "                for chunk in response.iter_content(chunk_size=1024): \n",
    "                    if chunk:\n",
    "                        f.write(chunk)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2542574",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_HVFHV_parquet_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cc8135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones(shapefile: str) -> gpd.GeoDataFrame:\n",
    "    data = gpd.read_file(shapefile)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d04c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_coords_for_taxi_zone_id(zone_loc_id, loaded_taxi_zones):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sample_size(population):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc33eed4-b2e9-4ab3-94a8-59f04e464c98",
   "metadata": {},
   "source": [
    "### Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_tlc_page(taxi_page):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_parquet_urls(all_urls):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Process Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_month(url):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(parquet_urls):\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    for parquet_url in parquet_urls:\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_month(parquet_url)\n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        \n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.contact(all_taxi_dataframes)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200776ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data():\n",
    "    all_urls = get_all_urls_from_taxi_page(TLC_URL)\n",
    "    all_parquet_urls = find_taxi_parquet_urls(all_urls)\n",
    "    taxi_data = get_and_clean_taxi_data(all_parquet_urls)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876bd645",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data = get_taxi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebd75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da7089-3f6b-4f93-a22e-76bf554daca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c85e25-6416-4c16-b98c-09596cdc6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07574983-f41d-4cd6-8f70-489493089b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_uber_month(url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches, processes, and cleans Uber trip data for a given month.\n",
    "\n",
    "    This function downloads Uber trip data from the specified URL, filters the data \n",
    "    based on predefined criteria, normalizes columns, and removes invalid or \n",
    "    out-of-bound data points. The processed data is returned as a cleaned DataFrame.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the Uber trip data file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned DataFrame containing filtered and normalized Uber trip data.\n",
    "    \"\"\"\n",
    "    \n",
    "    file_url = url.replace(' ', '')\n",
    "    name = file_url.split('trip-data/')[1]\n",
    "    data = pq.read_table(name).to_pandas()\n",
    "\n",
    "    #filter for Uber license\n",
    "    data = data[data['hvfhs_license_num'] == \"HV0003\"]\n",
    "\n",
    "    #sample the data\n",
    "    sample = int(calculate_sample_size(p=0.5, e=0.05, z=1.96))\n",
    "    data = data.sample(n=sample, random_state=1)\n",
    "\n",
    "    #add geographic coordinates for pickup and dropoff locations\n",
    "    if 'hvfhs_license_num' in data.columns:\n",
    "        coords = data.apply(\n",
    "            lambda row: pd.Series(\n",
    "                lookup_coords_for_taxi_zone_id(row['PULocationID'], loaded_taxi_zones) +\n",
    "                lookup_coords_for_taxi_zone_id(row['DOLocationID'], loaded_taxi_zones)\n",
    "            )\n",
    "            if (row['PULocationID'] in loaded_taxi_zones['LocationID'] and\n",
    "                row['DOLocationID'] in loaded_taxi_zones['LocationID'])\n",
    "            else pd.Series([None, None, None, None]),\n",
    "            axis=1\n",
    "    )\n",
    "    coords.columns = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "    data = pd.concat([data, coords], axis=1)\n",
    "    data = data.dropna(subset=['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude'])\n",
    "\n",
    "    #select columns\n",
    "    data = data[[\n",
    "             'request_datetime',\n",
    "             'pickup_datetime',\n",
    "             'dropoff_datetime',\n",
    "             'trip_miles',\n",
    "             'trip_time',\n",
    "             'pickup_longitude',\n",
    "             'pickup_latitude',\n",
    "             'dropoff_longitude',\n",
    "             'dropoff_latitude',\n",
    "             'base_passenger_fare',\n",
    "             'tolls',\n",
    "             'bcf',\n",
    "             'sales_tax',\n",
    "             'congestion_surcharge',\n",
    "             'airport_fee',\n",
    "             'tips'\n",
    "                ]\n",
    "            ] \n",
    "\n",
    "    #normalize appropriate column types - time\n",
    "    data['request_datetime'] = pd.to_datetime(data['request_datetime'])\n",
    "    data['pickup_datetime'] = pd.to_datetime(data['pickup_datetime'])\n",
    "    data['dropoff_datetime'] = pd.to_datetime(data['dropoff_datetime'])\n",
    "    \n",
    "    #normalize appropriate column types - number\n",
    "    data['trip_miles'] = pd.to_numeric(data['trip_miles'])\n",
    "    data['trip_time'] = pd.to_numeric(data['trip_time'])\n",
    "    data['base_passenger_fare'] = pd.to_numeric(data['base_passenger_fare'])\n",
    "    data['tolls'] = pd.to_numeric(data['tolls'])\n",
    "    data['bcf'] = pd.to_numeric(data['bcf'])\n",
    "    data['sales_tax'] = pd.to_numeric(data['sales_tax'])\n",
    "    data['congestion_surcharge'] = pd.to_numeric(data['congestion_surcharge'])\n",
    "    data['airport_fee'] = pd.to_numeric(data['airport_fee'])\n",
    "    data['tips'] = pd.to_numeric(data['tips'])\n",
    "    \n",
    "    #compute total surcharge\n",
    "    data['all_surcharge'] = data['congestion_surcharge'] + data['airport_fee'] + data['bcf'] + data['tips']\n",
    "    \n",
    "    #remove NaN\n",
    "    data = data.dropna()\n",
    "    \n",
    "    #normalize column names (pu, do, trip_time, bcf)\n",
    "    new_column_names = [\n",
    "             'request_datetime',\n",
    "             'pickup_datetime',\n",
    "             'dropoff_datetime',\n",
    "             'trip_miles',\n",
    "             'trip_time_seconds',\n",
    "             'pickup_longitude',\n",
    "             'pickup_latitude',\n",
    "             'dropoff_longitude',\n",
    "             'dropoff_latitude',\n",
    "             'base_passenger_fare',\n",
    "             'tolls',\n",
    "             'black_car_fund',\n",
    "             'sales_tax',\n",
    "             'congestion_surcharge',\n",
    "             'airport_fee',\n",
    "             'tips',\n",
    "             'all_surcharge'\n",
    "                       ]\n",
    "    data.columns = new_column_names\n",
    "\n",
    "    #remove invalid time\n",
    "    data = data[data['pickup_datetime'] < data['dropoff_datetime']]\n",
    "    #remove 0 distance\n",
    "    data = data[data['trip_miles'] > 0]\n",
    "    #remove 0 time\n",
    "    data = data[data['trip_time_seconds'] > 0]\n",
    "    \n",
    "    #removing trips that start and/or end outside (40.560445, -74.242330) and (40.908524, -73.717047)\n",
    "    data = data[(data['pickup_latitude'] >= 40.560445) & (data['pickup_latitude'] <= 40.908524) &\n",
    "                (data['pickup_longitude'] >= -74.242330) & (data['pickup_longitude'] <= -73.717047) &\n",
    "                (data['dropoff_latitude'] >= 40.560445) & (data['dropoff_latitude'] <= 40.908524) &\n",
    "                (data['dropoff_longitude'] >= -74.242330) & (data['dropoff_longitude'] <= -73.717047)]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_uber_data(parquet_urls: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Processes and cleans Uber trip data from a list of Parquet file URLs.\n",
    "\n",
    "    This function iterates through a list of Parquet file URLs, filters those related\n",
    "    to High Volume FHV, processes each month's data using`get_and_clean_uber_month`, \n",
    "    and combines the results into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "        parquet_urls (List[str]): A list of URLs pointing to Parquet files containing Uber data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned and combined DataFrame containing Uber trip data \n",
    "        from all specified Parquet files.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_uber_dataframes = []\n",
    "    \n",
    "    #process each URL in the list\n",
    "    for parquet_url in parquet_urls:\n",
    "        if ('fhvhv' in parquet_url):\n",
    "            dataframe = get_and_clean_uber_month(parquet_url)\n",
    "            all_uber_dataframes.append(dataframe)\n",
    "        \n",
    "    uber_data = pd.concat(all_uber_dataframes)\n",
    "    \n",
    "    return uber_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches and processes Uber trip data from TLC's website.\n",
    "\n",
    "    This function retrieves all URLs from the TLC page, filters for Parquet file URLs,\n",
    "    processes the Uber trip data from the relevant Parquet files, and returns a cleaned\n",
    "    and combined DataFrame.\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned DataFrame containing Uber trip data from all relevant Parquet files.\n",
    "    \"\"\"\n",
    "    all_urls = get_all_urls_from_tlc_page(TLC_URL)\n",
    "    all_parquet_urls = find_parquet_urls(all_urls)\n",
    "    taxi_data = get_and_clean_uber_data(all_parquet_urls)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339997e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d783db-e527-4847-bf70-2d7428ea3897",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fddeb14-cd70-4e83-8f93-974642c3bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    weather_csv_files = get_all_weather_csvs(WEATHER_CSV_DIR)\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "        \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48216557",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935261b7-ae23-427c-97ff-ea31aa4e44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dcb502-d1d1-447d-aa68-11bff0dc53b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f090eb94-a5b0-4d93-bf82-a596d2521b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c074aa3-a5f2-4586-8748-411e1e6c11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query either via sqlalchemy\n",
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_1)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "pd.read_sql(QUERY_1, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
